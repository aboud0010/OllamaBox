{===============================================================================
    ___  _ _                 ___
   / _ \| | |__ _ _ __  __ _| _ ) _____ __™
  | (_) | | / _` | '  \/ _` | _ \/ _ \ \ /
   \___/|_|_\__,_|_|_|_\__,_|___/\___/_\_\
  Embed & Control Ollama In Your Delphi App

 Copyright © 2025-present tinyBigGAMES™ LLC
 All Rights Reserved.

 https://github.com/tinyBigGAMES/OllamaBox

 See LICENSE file for license information
===============================================================================}

unit OllamaBox64;

interface

const
  COllamaBoxDLL = 'OllamaBox64.dll';

  CobStart      = 0;
  CobInProgress = 1;
  CobEnd        = 2;

type
  /// <summary>
  ///   A generic pointer representing an opaque handle to a TOllamaBox instance
  ///   managed by the OllamaBox DLL.
  /// </summary>
  /// <remarks>
  ///   This pointer type is used to abstract the internal structure of the object when
  ///   exposing it to foreign code or other languages. It must be created and released
  ///   using <see cref="obCreate"/> and <see cref="obFree"/>.
  /// </remarks>
  TOllamaBox = Pointer;

  /// <summary>
  ///   A simple callback procedure with no parameters, except for user-defined context.
  /// </summary>
  /// <param name="AUserData">
  ///   A pointer to user-defined data passed back when the callback is invoked.
  /// </param>
  /// <remarks>
  ///   Use this for lifecycle events like thinking start/end or response start/end.
  /// </remarks>
  TobCallback = procedure(const AUserData: Pointer); stdcall;

  /// <summary>
  ///   A callback function used to check whether the current operation should be cancelled.
  /// </summary>
  /// <param name="AUserData">
  ///   A pointer to user-defined data passed back to determine cancellation contextually.
  /// </param>
  /// <returns>
  ///   <c>True</c> if the operation should be cancelled; otherwise, <c>False</c>.
  /// </returns>
  /// <remarks>
  ///   This callback is invoked periodically during inference and model pulls
  ///   to support user interruption or timeout logic.
  /// </remarks>
  TobCancelCallback = function(const AUserData: Pointer): Boolean; stdcall;

  /// <summary>
  ///   A callback procedure that is invoked for each token generated by the model.
  /// </summary>
  /// <param name="AToken">
  ///   A pointer to a null-terminated wide string representing the current output token.
  /// </param>
  /// <param name="AUserData">
  ///   A pointer to user-defined context data.
  /// </param>
  /// <remarks>
  ///   This callback enables token-by-token streaming and real-time output display.
  ///   The token is provided as UTF-16 (PWideChar) for wide platform compatibility.
  /// </remarks>
  TobNextTokenCallback = procedure(
    const AToken: PWideChar;
    const AUserData: Pointer
  ); stdcall;

  /// <summary>
  ///   A callback procedure that provides real-time status updates during model downloading.
  /// </summary>
  /// <param name="AMessage">
  ///   A pointer to a null-terminated wide string describing the current download step.
  /// </param>
  /// <param name="APercent">
  ///   A floating-point percentage (0.0 to 100.0) indicating download progress.
  /// </param>
  /// <param name="AStatus">
  ///   A numeric status code representing the operation phase:
  ///   <c>0 = Start</c>, <c>1 = In Progress</c>, <c>2 = End</c>.
  /// </param>
  /// <param name="AUserData">
  ///   A pointer to user-defined data associated with the download session.
  /// </param>
  /// <remarks>
  ///   This callback provides structured progress feedback to the caller,
  ///   allowing for integration with progress bars, logs, or UI updates.
  /// </remarks>
  TobPullModelCallback = procedure(
    const AMessage: PWideChar;
    const APercent: Double;
    const AStatus: UInt32;
    const AUserData: Pointer
  ); stdcall;

/// <summary>
///   Creates and returns a new instance of the <see cref="TOllamaBox"/> class from the external DLL.
/// </summary>
/// <returns>
///   A newly allocated <see cref="TOllamaBox"/> instance managed externally via the OllamaBox DLL.
/// </returns>
/// <remarks>
///   This function is declared with <c>stdcall</c> calling convention for compatibility with
///   external languages such as C, C++, or other environments interfacing with the DLL.
///   The returned object must be freed using <see cref="obFree"/>.
/// </remarks>
function obCreate(): TOllamaBox; stdcall; external COllamaBoxDLL;

/// <summary>
///   Releases a previously created <see cref="TOllamaBox"/> instance.
/// </summary>
/// <param name="AOllamaBox">
///   A reference to the <see cref="TOllamaBox"/> instance to free.
/// </param>
/// <remarks>
///   After calling this function, the passed-in reference is invalid and should not be accessed.
///   This is the counterpart to <see cref="obCreate"/> and must be used to avoid memory leaks
///   when working with the OllamaBox DLL from outside Delphi.
/// </remarks>
procedure obFree(var AOllamaBox: TOllamaBox); stdcall; external COllamaBoxDLL;

/// <summary>
///   Registers a cancellation callback for the specified <see cref="TOllamaBox"/> instance.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the TOllamaBox instance returned by <see cref="obCreate"/>.
/// </param>
/// <param name="AHandler">
///   A callback function of type <see cref="TobCancelCallback"/> to be called during inference or pull operations.
/// </param>
/// <param name="AUserData">
///   A user-defined pointer that will be passed back to the callback when invoked.
/// </param>
/// <remarks>
///   The cancellation callback allows external code to interrupt long-running operations
///   such as model generation or download. The callback will be polled periodically,
///   and should return <c>True</c> to abort the current process.
/// </remarks>
procedure obSetOnCancel(
  const AOllamaBox: TOllamaBox;
  const AHandler: TobCancelCallback;
  const AUserData: Pointer
); stdcall; external COllamaBoxDLL;

/// <summary>
///   Registers a streaming token callback for the specified <see cref="TOllamaBox"/> instance.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the TOllamaBox instance returned by <see cref="obCreate"/>.
/// </param>
/// <param name="AHandler">
///   A callback procedure of type <see cref="TobNextTokenCallback"/> that will be invoked as each token is generated.
/// </param>
/// <param name="AUserData">
///   A user-defined pointer that will be passed to the callback on each invocation.
/// </param>
/// <remarks>
///   This callback is called once per generated token during the model's response phase.
///   It enables real-time streaming, progressive display, or external output processing.
/// </remarks>
procedure obSetOnNextToken(
  const AOllamaBox: TOllamaBox;
  const AHandler: TobNextTokenCallback;
  const AUserData: Pointer
); stdcall; external COllamaBoxDLL;

/// <summary>
///   Registers a callback to be invoked immediately before the model begins its "thinking" phase.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="AHandler">
///   A procedure of type <see cref="TobCallback"/> to be called at the start of thinking.
/// </param>
/// <param name="AUserData">
///   A pointer to user-defined data that will be passed to the callback when invoked.
/// </param>
/// <remarks>
///   This callback is typically used to update the UI or prepare the application
///   for upcoming token generation. It fires just before inference begins.
/// </remarks>
procedure obSetOnThinkStart(
  const AOllamaBox: TOllamaBox;
  const AHandler: TobCallback;
  const AUserData: Pointer
); stdcall; external COllamaBoxDLL;

/// <summary>
///   Registers a callback to be invoked immediately after the model completes its "thinking" phase.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="AHandler">
///   A procedure of type <see cref="TobCallback"/> to be called when thinking ends.
/// </param>
/// <param name="AUserData">
///   A pointer to user-defined data that will be passed to the callback when invoked.
/// </param>
/// <remarks>
///   This callback is typically used to conclude loading indicators or transition the application
///   into the response streaming phase. It is triggered right before tokens begin streaming.
/// </remarks>
procedure obSetOnThinkEnd(
  const AOllamaBox: TOllamaBox;
  const AHandler: TobCallback;
  const AUserData: Pointer
); stdcall; external COllamaBoxDLL;

/// <summary>
///   Registers a callback to be invoked just before the model begins streaming its response.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="AHandler">
///   A procedure of type <see cref="TobCallback"/> to be executed at the start of the response phase.
/// </param>
/// <param name="AUserData">
///   A pointer to user-defined data passed back to the callback.
/// </param>
/// <remarks>
///   This callback occurs immediately after the "thinking" phase ends and before the first token
///   is emitted by the model. It is commonly used to prepare the UI for output, such as starting
///   visual feedback or enabling a response container.
/// </remarks>
procedure obSetOnResponseStart(
  const AOllamaBox: TOllamaBox;
  const AHandler: TobCallback;
  const AUserData: Pointer
); stdcall; external COllamaBoxDLL;

/// <summary>
///   Registers a callback to be invoked after the model finishes generating its response.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="AHandler">
///   A procedure of type <see cref="TobCallback"/> to be executed at the end of the response phase.
/// </param>
/// <param name="AUserData">
///   A pointer to user-defined data passed back to the callback.
/// </param>
/// <remarks>
///   This callback is triggered when the model has finished generating all tokens
///   or the operation has been cancelled. It can be used to finalize UI updates,
///   hide streaming indicators, or enable follow-up actions.
/// </remarks>
procedure obSetOnResponseEnd(
  const AOllamaBox: TOllamaBox;
  const AHandler: TobCallback;
  const AUserData: Pointer
); stdcall; external COllamaBoxDLL;

  /// <summary>
  ///   Registers a callback for receiving progress updates during a model pull operation from the Ollama registry.
  /// </summary>
  /// <param name="AOllamaBox">
  ///   A handle to the <see cref="TOllamaBox"/> instance.
  /// </param>
  /// <param name="AHandler">
  ///   A procedure of type <see cref="TobPullModelCallback"/> to receive pull progress, status messages, and phase updates.
  /// </param>
  /// <param name="AUserData">
  ///   A user-defined pointer that will be passed to the callback for contextual reference.
  /// </param>
  /// <remarks>
  ///   This callback is invoked during model download and setup operations.
  ///   It allows the application to show progress bars, log messages, or handle cancellation UI.
  /// </remarks>
  procedure obSetOnPullModel(
    const AOllamaBox: TOllamaBox;
    const AHandler: TobPullModelCallback;
    const AUserData: Pointer
  ); stdcall; external COllamaBoxDLL;

/// <summary>
///   Returns the SemVer version string of the TOllamaBox wrapper.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   A pointer to a null-terminated wide string (UTF-16) containing the wrapper version (e.g., <c>"1.0.0"</c>).
/// </returns>
/// <remarks>
///   This version reflects the version of the OllamaBox library itself, not the Ollama runtime.
///   It follows Semantic Versioning: MAJOR.MINOR.PATCH.
/// </remarks>
function obGetVersion(const AOllamaBox: TOllamaBox): PWideChar; stdcall; external COllamaBoxDLL;

/// <summary>
///   Returns the version string of the Ollama runtime currently installed or running.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   A pointer to a null-terminated wide string (UTF-16) representing the Ollama runtime version (e.g., <c>"0.1.34"</c>).
/// </returns>
/// <remarks>
///   This version is retrieved from the Ollama server itself, and can be used to check compatibility
///   between the host wrapper and the runtime backend.
/// </remarks>
function obGetOllamaVersion(const AOllamaBox: TOllamaBox): PWideChar; stdcall; external COllamaBoxDLL;

/// <summary>
///   Displays the ASCII logo to the active console using the specified text color.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="AColor">
///   A pointer to a null-terminated wide string representing the desired color name (e.g., <c>"Green"</c>, <c>"Cyan"</c>).
/// </param>
/// <remarks>
///   The color must match a recognized console color name based on the CIS (Common Interface Specification) color set.
///   If the color is invalid or unsupported, a default console color will be used.
///   This function writes directly to the standard output stream.
/// </remarks>
procedure obDisplayLogo(
  const AOllamaBox: TOllamaBox;
  const AColor: PWideChar
); stdcall; external COllamaBoxDLL;

/// <summary>
///   Downloads and extracts the Ollama server binary from its official GitHub release.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <remarks>
///   This method retrieves the latest Ollama release, stores it in the <c>ServerDownloadPath</c>,
///   and extracts the server executable to <c>ServerPath</c>.
///   It is required before calling <see cref="obStartServer"/> unless the server has already been downloaded.
/// </remarks>
procedure obDownloadServer(
  const AOllamaBox: TOllamaBox
); stdcall; external COllamaBoxDLL;

/// <summary>
///   Starts the embedded Ollama server in the current application process.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   <c>True</c> if the server was successfully started; otherwise, <c>False</c>.
/// </returns>
/// <remarks>
///   The server executable must already be present at the path specified by <c>ServerPath</c>,
///   typically via a prior call to <see cref="obDownloadServer"/>. This method launches the Ollama
///   server within the same process context without creating an external service or detached process.
/// </remarks>
function obStartServer(const AOllamaBox: TOllamaBox): Boolean; stdcall; external COllamaBoxDLL;

/// <summary>
///   Determines whether the Ollama server has already been started in the current process.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   <c>True</c> if the Ollama server is currently running inside the application; otherwise, <c>False</c>.
/// </returns>
/// <remarks>
///   This method checks the internal state of the process and does not inspect system services
///   or external processes. Use it to verify whether <see cref="obStartServer"/> has already been called.
/// </remarks>
function obServerStarted(const AOllamaBox: TOllamaBox): Boolean; stdcall; external COllamaBoxDLL;

/// <summary>
///   Stops the embedded Ollama server running inside the current application process.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <remarks>
///   This method terminates the locally started Ollama server that was launched using
///   <see cref="obStartServer"/>. It has no effect if the server is not running.
///   Resources associated with the server will be released.
/// </remarks>
procedure obStopServer(const AOllamaBox: TOllamaBox); stdcall; external COllamaBoxDLL;

/// <summary>
///   Checks whether the Ollama server is running and accessible via its local API endpoint.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   <c>True</c> if the server is running and responding to requests at its base URL; otherwise, <c>False</c>.
/// </returns>
/// <remarks>
///   Unlike <see cref="obServerStarted"/>, this method performs a connectivity check
///   (e.g., ping or version request) against the server endpoint returned by <see cref="obGetServerBaseAPIUrl"/>.
/// </remarks>
function obServerRunning(const AOllamaBox: TOllamaBox): Boolean; stdcall; external COllamaBoxDLL;

/// <summary>
///   Returns the base API URL used to communicate with the embedded Ollama server.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   A pointer to a null-terminated wide string containing the full base URL
///   (e.g., <c>"http://localhost:11434"</c>).
/// </returns>
/// <remarks>
///   This URL includes the host and port defined by the <c>ServerPort</c> property
///   and is used as the root endpoint for all local API requests.
/// </remarks>
function obGetServerBaseAPIUrl(const AOllamaBox: TOllamaBox): PWideChar; stdcall; external COllamaBoxDLL;

/// <summary>
///   Clears all system prompts from the current conversation context.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <remarks>
///   This removes all previously added system instructions that guide the model's behavior.
///   After calling this method, the model will no longer be influenced by prior system-level directives
///   unless new ones are added using <see cref="obAddSystem"/>.
/// </remarks>
procedure obClearSystem(const AOllamaBox: TOllamaBox); stdcall; external COllamaBoxDLL;

/// <summary>
///   Adds a system prompt to the conversation context, which influences the model's behavior.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="AText">
///   A null-terminated wide string containing the system instruction or behavioral directive.
/// </param>
/// <returns>
///   The zero-based index of the newly added system prompt.
/// </returns>
/// <remarks>
///   System prompts are used to establish rules, persona, tone, or formatting expectations.
///   They are submitted to the model before user prompts during inference.
/// </remarks>
function obAddSystem(const AOllamaBox: TOllamaBox; const AText: PWideChar): UInt32; stdcall; external COllamaBoxDLL;

/// <summary>
///   Returns the number of system prompts currently added to the conversation context.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   A 32-bit unsigned integer representing the total number of active system prompts.
/// </returns>
/// <remarks>
///   This reflects the count of prompts added using <see cref="obAddSystem"/>.
/// </remarks>
function obSystemCount(const AOllamaBox: TOllamaBox): UInt32; stdcall; external COllamaBoxDLL;

/// <summary>
///   Removes a specific system prompt by index from the conversation context.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="AIndex">
///   The zero-based index of the system prompt to remove.
/// </param>
/// <remarks>
///   If the specified index is out of bounds, the call may be ignored or raise an internal error,
///   depending on the implementation. Use <see cref="obSystemCount"/> to validate the index range.
/// </remarks>
procedure obRemoveSystem(const AOllamaBox: TOllamaBox; const AIndex: UInt32); stdcall; external COllamaBoxDLL;

/// <summary>
///   Returns the concatenated content of all active system prompts.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   A pointer to a null-terminated wide string containing all system prompts combined into one block.
/// </returns>
/// <remarks>
///   This is useful for displaying, logging, or exporting the currently active system instructions
///   in their full concatenated form.
/// </remarks>
function obGetSystem(const AOllamaBox: TOllamaBox): PWideChar; stdcall; external COllamaBoxDLL;

/// <summary>
///   Clears the current token-based conversation context.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <remarks>
///   This removes all prior input tokens, system prompts, and response history
///   from memory, effectively resetting the session.
///   Use this when beginning a new, unrelated interaction with the model.
/// </remarks>
procedure obClearContext(const AOllamaBox: TOllamaBox); stdcall; external COllamaBoxDLL;

/// <summary>
///   Saves the current model context (tokens, system, history) to a file.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="AFilename">
///   A null-terminated wide string representing the path to the context file to create.
/// </param>
/// <returns>
///   <c>True</c> if the context was successfully written to disk; otherwise, <c>False</c>.
/// </returns>
/// <remarks>
///   The context file includes model token memory and may also contain system and prompt structure,
///   depending on implementation. It can be restored later using <see cref="obLoadContext"/>.
/// </remarks>
function obSaveContext(const AOllamaBox: TOllamaBox; const AFilename: PWideChar): Boolean; stdcall; external COllamaBoxDLL;

/// <summary>
///   Loads a previously saved conversation context from a file.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="AFilename">
///   A null-terminated wide string representing the path to the context file to load.
/// </param>
/// <returns>
///   <c>True</c> if the context was successfully loaded; otherwise, <c>False</c>.
/// </returns>
/// <remarks>
///   This restores the full model session state, allowing inference to continue from
///   where it previously left off. Only context files generated by <see cref="obSaveContext"/>
///   are guaranteed to be compatible.
/// </remarks>
function obLoadContext(const AOllamaBox: TOllamaBox; const AFilename: PWideChar): Boolean; stdcall; external COllamaBoxDLL;

/// <summary>
///   Clears all previously added image inputs from the current conversation context.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <remarks>
///   This resets the internal image array, removing all images that were added
///   using <see cref="obAddImage"/>. It is typically called before starting a new
///   multimodal inference request.
/// </remarks>
procedure obClearImages(const AOllamaBox: TOllamaBox); stdcall; external COllamaBoxDLL;

/// <summary>
///   Adds an image to the current input context for use with multimodal models that support image input.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="AFilename">
///   A null-terminated wide string containing the full path to the image file to be added.
/// </param>
/// <returns>
///   <c>True</c> if the image was successfully loaded and processed; otherwise, <c>False</c>.
/// </returns>
/// <remarks>
///   The image will be converted to a base64-encoded representation internally
///   and attached to the input for use with vision-capable models like LLaVA, Gemma-Vision, etc.
///   Supported formats typically include PNG and JPEG.
/// </remarks>
function obAddImage(const AOllamaBox: TOllamaBox; const AFilename: PWideChar): Boolean; stdcall; external COllamaBoxDLL;

/// <summary>
///   Pulls the specified model from the Ollama model registry and prepares it for local use.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   <c>True</c> if the model was successfully pulled and initialized; otherwise, <c>False</c>.
/// </returns>
/// <remarks>
///   The model name is defined via the <c>Model</c> property on the instance.
///   This function downloads and verifies the model, making it ready for inference.
///   Progress can be monitored using <see cref="obSetOnPullModel"/>.
/// </remarks>
function obPull(const AOllamaBox: TOllamaBox): Boolean; stdcall; external COllamaBoxDLL;

/// <summary>
///   Performs text generation using the currently set model, prompt, and input context.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   <c>True</c> if a response was successfully generated; otherwise, <c>False</c>.
/// </returns>
/// <remarks>
///   The model, prompt, system prompts, images, and other context data must be configured before calling this method.
///   Tokens are streamed using <see cref="obSetOnNextToken"/>, and lifecycle events
///   like <see cref="obSetOnThinkStart"/> and <see cref="obSetOnResponseStart"/> are triggered if set.
/// </remarks>
function obGenerate(const AOllamaBox: TOllamaBox): Boolean; stdcall; external COllamaBoxDLL;

/// <summary>
///   Performs a web search using the given query and returns summarized results.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="AQuery">
///   A null-terminated wide string containing the search query text.
/// </param>
/// <returns>
///   A pointer to a null-terminated wide string containing the plain text search results.
/// </returns>
/// <remarks>
///   This function connects to an external search provider (e.g., Tavily) to retrieve
///   real-time web content relevant to the query. It is useful for enhancing inference
///   with up-to-date or externally sourced information in RAG workflows.<br/><br/>
///   Requires an active internet connection and a valid API key (configured internally).
/// </remarks>
function obWebSearch(const AOllamaBox: TOllamaBox; const AQuery: PWideChar): PWideChar; stdcall; external COllamaBoxDLL;

/// <summary>
///   Retrieves the TCP port number configured for the embedded Ollama server.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   A 32-bit unsigned integer representing the port number (e.g., <c>11434</c>).
/// </returns>
/// <remarks>
///   This is the port on which the Ollama server will listen for local HTTP API requests.
///   It must match the value used to construct the server's base URL (see <see cref="obGetServerBaseAPIUrl"/>).
/// </remarks>
function obGetServerPort(const AOllamaBox: TOllamaBox): UInt32; stdcall; external COllamaBoxDLL;

/// <summary>
///   Sets the TCP port number for the embedded Ollama server.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="APort">
///   A 32-bit unsigned integer representing the desired port (e.g., <c>11434</c>).
/// </param>
/// <remarks>
///   This must be set before calling <see cref="obStartServer"/>. If the specified port is already
///   in use or invalid, server startup may fail. Use port numbers in the valid TCP range (typically above 1024).
/// </remarks>
procedure obSetServerPort(const AOllamaBox: TOllamaBox; const APort: UInt32); stdcall; external COllamaBoxDLL;

/// <summary>
///   Retrieves the current file system path to the model used for inference.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   A pointer to a null-terminated wide string containing the model file or directory path.
/// </returns>
/// <remarks>
///   This path points to a local GGUF model file or a directory managed by Ollama.
///   It is determined automatically after a successful <see cref="obPull"/> or can be manually assigned.
/// </remarks>
function obGetModelPath(const AOllamaBox: TOllamaBox): PWideChar; stdcall; external COllamaBoxDLL;

/// <summary>
///   Sets the file system path to the local model file or directory to be used for inference.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="APath">
///   A null-terminated wide string containing the full path to a local GGUF file or model directory.
/// </param>
/// <remarks>
///   This allows manually specifying a local model instead of pulling one from the Ollama registry.
///   The model at this path will be used for subsequent inference operations.
/// </remarks>
procedure obSetModelPath(const AOllamaBox: TOllamaBox; const APath: PWideChar); stdcall; external COllamaBoxDLL;

/// <summary>
///   Retrieves the file system path where the Ollama server executable is located.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   A pointer to a null-terminated wide string containing the server path.
/// </returns>
/// <remarks>
///   This is the folder where <see cref="obDownloadServer"/> extracts the Ollama server binary.
///   It is also the location from which <see cref="obStartServer"/> will launch the server.
/// </remarks>
function obGetServerPath(const AOllamaBox: TOllamaBox): PWideChar; stdcall; external COllamaBoxDLL;

/// <summary>
///   Sets the file system path where the Ollama server should be extracted and run.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="APath">
///   A null-terminated wide string specifying the target directory for the Ollama server binary.
/// </param>
/// <remarks>
///   This path must be writable and must be set before calling <see cref="obDownloadServer"/> or <see cref="obStartServer"/>.
///   It determines where the server files will be unpacked and run.
/// </remarks>
procedure obSetServerPath(const AOllamaBox: TOllamaBox; const APath: PWideChar); stdcall; external COllamaBoxDLL;

/// <summary>
///   Retrieves the file system path where the Ollama server ZIP file will be downloaded.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   A pointer to a null-terminated wide string containing the temporary download path.
/// </returns>
/// <remarks>
///   This path is used by <see cref="obDownloadServer"/> to store the server archive
///   before it is extracted to the final <see cref="obGetServerPath"/> location.
/// </remarks>
function obGetServerDownloadPath(const AOllamaBox: TOllamaBox): PWideChar; stdcall; external COllamaBoxDLL;

/// <summary>
///   Sets the path where the Ollama server ZIP file should be downloaded before extraction.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="APath">
///   A null-terminated wide string specifying the temporary folder for server downloads.
/// </param>
/// <remarks>
///   This directory must be writable. If the file already exists, it may be overwritten.
///   After download and extraction, the ZIP file may be deleted depending on internal cleanup behavior.
/// </remarks>
procedure obSetServerDownloadPath(const AOllamaBox: TOllamaBox; const APath: PWideChar); stdcall; external COllamaBoxDLL;

/// <summary>
///   Retrieves the model name currently assigned to the <see cref="TOllamaBox"/> instance.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   A pointer to a null-terminated wide string containing the model name (e.g., <c>"llama3"</c>, <c>"gemma:7b"</c>).
/// </returns>
/// <remarks>
///   This is the value used by <see cref="obPull"/> to locate and download the appropriate model from the Ollama registry.
/// </remarks>
function obGetModel(const AOllamaBox: TOllamaBox): PWideChar; stdcall; external COllamaBoxDLL;

/// <summary>
///   Sets the model name to be used for inference and pulling from the Ollama registry.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="AModel">
///   A null-terminated wide string representing the model name or tag (e.g., <c>"llama3:latest"</c>).
/// </param>
/// <remarks>
///   The model name must be a valid identifier recognized by the Ollama registry.
///   This value is required before calling <see cref="obPull"/> or <see cref="obGenerate"/>.
/// </remarks>
procedure obSetModel(const AOllamaBox: TOllamaBox; const AModel: PWideChar); stdcall; external COllamaBoxDLL;

/// <summary>
///   Retrieves the current prompt text assigned to the <see cref="TOllamaBox"/> instance.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   A pointer to a null-terminated wide string representing the prompt text.
/// </returns>
/// <remarks>
///   This is the user input or instruction that will be submitted to the model
///   when <see cref="obGenerate"/> is called. It may be a question, task instruction,
///   or continuation depending on the application.
/// </remarks>
function obGetPrompt(const AOllamaBox: TOllamaBox): PWideChar; stdcall; external COllamaBoxDLL;

/// <summary>
///   Sets the prompt text to be used for the next inference operation.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="APrompt">
///   A null-terminated wide string representing the prompt to submit to the model.
/// </param>
/// <remarks>
///   The prompt is used as the main input during a call to <see cref="obGenerate"/>.
///   It may include question content, formatting instructions, or appended context.
///   System prompts and images (if used) are processed alongside this main prompt.
/// </remarks>
procedure obSetPrompt(const AOllamaBox: TOllamaBox; const APrompt: PWideChar); stdcall; external COllamaBoxDLL;

/// <summary>
///   Retrieves the current keep-alive setting for the embedded Ollama server.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   An integer value representing the keep-alive behavior:
///   <list type="bullet">
///     <item><c>0</c> – shut down immediately after each operation</item>
///     <item><c>> 0</c> – keep alive for N seconds of inactivity</item>
///     <item><c>&lt; 0</c> – keep model in VRAM indefinitely</item>
///   </list>
/// </returns>
/// <remarks>
///   This setting controls how long the server or model remains loaded between operations,
///   affecting startup delay and GPU memory usage.
/// </remarks>
function obGetKeepAlive(const AOllamaBox: TOllamaBox): Int32; stdcall; external COllamaBoxDLL;

/// <summary>
///   Sets the keep-alive timeout for the embedded Ollama server.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="AKeepAlive">
///   An integer value controlling idle behavior:
///   <list type="bullet">
///     <item><c>0</c> – terminate server immediately after use</item>
///     <item><c>60</c> – keep alive for 60 seconds after last use</item>
///     <item><c>-1</c> – keep model persistently loaded in VRAM</item>
///   </list>
/// </param>
/// <remarks>
///   This setting is useful for optimizing performance when reusing models across multiple requests.
///   It should be set before calling <see cref="obStartServer"/>.
/// </remarks>
procedure obSetKeepAlive(const AOllamaBox: TOllamaBox; const AKeepAlive: Int32); stdcall; external COllamaBoxDLL;

/// <summary>
///   Retrieves the index of the main GPU device configured for model inference.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   An integer indicating the GPU index to use:
///   <list type="bullet">
///     <item><c>-1</c> – automatically select the best available GPU</item>
///     <item><c>0</c> or greater – use a specific GPU by index</item>
///   </list>
/// </returns>
/// <remarks>
///   GPU indexing is platform-dependent and may vary based on installed hardware.
///   This value must be set before model execution for it to take effect.
/// </remarks>
function obGetMainGPU(const AOllamaBox: TOllamaBox): Int32; stdcall; external COllamaBoxDLL;

/// <summary>
///   Sets the index of the GPU device to use as the primary accelerator for model inference.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="AMainGPU">
///   An integer indicating the target GPU:
///   <list type="bullet">
///     <item><c>-1</c> – auto-select the best available GPU</item>
///     <item><c>0</c> – use the first GPU</item>
///     <item><c>1</c>, <c>2</c>, etc. – use specific GPU by index</item>
///   </list>
/// </param>
/// <remarks>
///   This should be configured before model load or inference begins.
///   If the selected GPU is unavailable or unsupported, fallback behavior is implementation-defined.
/// </remarks>
procedure obSetMainGPU(const AOllamaBox: TOllamaBox; const AMainGPU: Int32); stdcall; external COllamaBoxDLL;

/// <summary>
///   Retrieves the number of model layers currently configured to run on the GPU.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   An integer representing the number of transformer layers offloaded to the GPU:
///   <list type="bullet">
///     <item><c>-1</c> – all eligible layers are assigned to the GPU</item>
///     <item><c>0</c> – all layers run on CPU only</item>
///     <item><c>1..N</c> – only the first N layers are offloaded to the GPU</item>
///   </list>
/// </returns>
/// <remarks>
///   This value determines how much of the model is accelerated by GPU hardware,
///   directly affecting performance and VRAM usage.
/// </remarks>
function obGetGPULayers(const AOllamaBox: TOllamaBox): Int32; stdcall; external COllamaBoxDLL;

/// <summary>
///   Sets the number of model layers to offload to the GPU for inference acceleration.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="AGPULayers">
///   An integer controlling layer usage:
///   <list type="bullet">
///     <item><c>-1</c> – use GPU for all eligible layers</item>
///     <item><c>0</c> – use CPU only</item>
///     <item><c>1..N</c> – offload only the first N layers to GPU</item>
///   </list>
/// </param>
/// <remarks>
///   Higher values offer better performance but consume more VRAM.
///   This setting must be configured before model load or inference begins.
/// </remarks>
procedure obSetGPULayer(const AOllamaBox: TOllamaBox; const AGPULayers: Int32); stdcall; external COllamaBoxDLL;

/// <summary>
///   Retrieves the maximum number of context tokens configured for inference.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   A 32-bit unsigned integer indicating the total context window size in tokens.
/// </returns>
/// <remarks>
///   This value defines how many tokens (prompt + generated) can be retained in memory
///   during inference. The value must be compatible with the model’s maximum capacity
///   and VRAM limits when using GPU acceleration.
/// </remarks>
function obGetMaxContext(const AOllamaBox: TOllamaBox): UInt32; stdcall; external COllamaBoxDLL;

/// <summary>
///   Sets the maximum number of context tokens the model should use for inference.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="AMaxContext">
///   A 32-bit unsigned integer representing the desired context window size (in tokens).
/// </param>
/// <remarks>
///   This setting affects how much of the conversation history the model can "remember" during generation.
///   Larger values increase memory and VRAM requirements and must not exceed the model's maximum context limit.
/// </remarks>
procedure obSetMaxContext(const AOllamaBox: TOllamaBox; const AMaxContext: UInt32); stdcall; external COllamaBoxDLL;

/// <summary>
///   Retrieves the suffix text that will be appended after the model’s generated response.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   A pointer to a null-terminated wide string representing the suffix text.
/// </returns>
/// <remarks>
///   The suffix is added after the last generated token and is not part of the prompt
///   or model inference context. It is often used for formatting, such as line breaks,
///   punctuation, or custom delimiters.
/// </remarks>
function obGetSuffix(const AOllamaBox: TOllamaBox): PWideChar; stdcall; external COllamaBoxDLL;

/// <summary>
///   Sets the suffix text to append after the model's response is fully generated.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="ASuffix">
///   A null-terminated wide string representing the suffix text to append (e.g., <c>"\n"</c> or <c>"</end>"</c>).
/// </param>
/// <remarks>
///   This suffix is applied after generation is complete and is not included in the prompt or token count.
///   It is useful for terminal output, formatting in logs, or signaling end-of-response to a downstream parser.
/// </remarks>
procedure obSetSuffix(const AOllamaBox: TOllamaBox; const ASuffix: PWideChar); stdcall; external COllamaBoxDLL;

/// <summary>
///   Retrieves the current temperature value used for controlling token sampling randomness.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   A single-precision floating-point value representing the temperature (e.g., <c>0.8</c>).
/// </returns>
/// <remarks>
///   Temperature affects the variability of the model's output:
///   <list type="bullet">
///     <item><c>0.0</c> – deterministic output</item>
///     <item><c>~0.7</c> – balanced creativity and coherence</item>
///     <item><c>1.0+</c> – high randomness and diversity</item>
///   </list>
/// </remarks>
function obGetTemperature(const AOllamaBox: TOllamaBox): Single; stdcall; external COllamaBoxDLL;

/// <summary>
///   Sets the temperature value used to control the randomness of model output.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="ATemperature">
///   A single-precision float indicating the desired temperature (e.g., <c>0.7</c>).
/// </param>
/// <remarks>
///   Higher temperatures produce more varied and creative results, while lower values
///   yield more focused and deterministic output. A typical default is <c>0.7</c>.
///   Values below <c>0.0</c> may be clamped or treated as <c>0</c>.
/// </remarks>
procedure obSetTemperature(const AOllamaBox: TOllamaBox; const ATemperature: Single); stdcall; external COllamaBoxDLL;

/// <summary>
///   Retrieves the random seed currently set for model inference.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   A signed 32-bit integer representing the seed value.
/// </returns>
/// <remarks>
///   The seed determines the randomness of token sampling. When the same prompt and parameters
///   are used with the same seed, the output will be deterministic.
///   A negative value (e.g., <c>-1</c>) indicates that a random seed will be used.
/// </remarks>
function obGetSeed(const AOllamaBox: TOllamaBox): Int32; stdcall; external COllamaBoxDLL;

/// <summary>
///   Sets the random seed used for inference token generation.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="ASeed">
///   A signed 32-bit integer to use as the seed:
///   <list type="bullet">
///     <item><c>&gt;= 0</c> – use this exact seed for reproducible output</item>
///     <item><c>&lt; 0</c> – use a randomly generated seed each time</item>
///   </list>
/// </param>
/// <remarks>
///   Seeds allow for deterministic behavior when testing or comparing outputs,
///   and non-deterministic behavior when seeking diverse results.
/// </remarks>
procedure obSetSeed(const AOllamaBox: TOllamaBox; const ASeed: Int32); stdcall; external COllamaBoxDLL;

/// <summary>
///   Retrieves the number of CPU threads currently configured for model inference.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   A signed 32-bit integer indicating the number of threads to use:
///   <list type="bullet">
///     <item><c>0</c> or less – auto-select optimal number of threads</item>
///     <item><c>1..N</c> – fixed number of CPU threads</item>
///   </list>
/// </returns>
/// <remarks>
///   This value controls parallelism during model execution and directly affects
///   CPU-bound performance. If GPU acceleration is used, fewer threads may be necessary.
/// </remarks>
function obGetThreads(const AOllamaBox: TOllamaBox): Int32; stdcall; external COllamaBoxDLL;

/// <summary>
///   Sets the number of CPU threads to use for model inference.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="AThreads">
///   A signed 32-bit integer specifying the thread count:
///   <list type="bullet">
///     <item><c>0</c> or negative – auto-detect optimal thread count</item>
///     <item><c>1..N</c> – manually specify thread usage</item>
///   </list>
/// </param>
/// <remarks>
///   This should be set before inference begins. Higher values can improve performance
///   on multi-core systems, but may increase CPU usage and memory pressure.
/// </remarks>
procedure obSetThreads(const AOllamaBox: TOllamaBox; const AThreads: Int32); stdcall; external COllamaBoxDLL;

/// <summary>
///   Retrieves the full response text generated by the model during the last inference operation.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   A pointer to a null-terminated wide string containing the full generated response.
/// </returns>
/// <remarks>
///   This response is collected after calling <see cref="obGenerate"/> and reflects
///   the final assembled output after all tokens have been processed.<br/>
///   If streaming output via <see cref="obSetOnNextToken"/> was used, this contains the concatenated result.
/// </remarks>
function obGetResponse(const AOllamaBox: TOllamaBox): PWideChar; stdcall; external COllamaBoxDLL;

/// <summary>
///   Returns the number of input tokens used in the most recent inference operation.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   The number of tokens used for the prompt, system messages, and context.
/// </returns>
/// <remarks>
///   Useful for measuring prompt size and tracking model usage costs.
/// </remarks>
function obGetInputTokens(const AOllamaBox: TOllamaBox): UInt32; stdcall; external COllamaBoxDLL;

/// <summary>
///   Returns the number of tokens generated by the model during the last inference.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   The number of output tokens produced by the model.
/// </returns>
/// <remarks>
///   This does not include any manually appended suffix text.
/// </remarks>
function obGetOutputTokens(const AOllamaBox: TOllamaBox): UInt32; stdcall; external COllamaBoxDLL;

/// <summary>
///   Returns the total number of tokens processed during the last inference session.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   The combined count of input and output tokens.
/// </returns>
/// <remarks>
///   Equivalent to <c>InputTokens + OutputTokens</c>.
///   Helpful for logging, billing estimation, or session history.
/// </remarks>
function obGetTotalTokens(const AOllamaBox: TOllamaBox): UInt32; stdcall; external COllamaBoxDLL;

/// <summary>
///   Returns the speed of token generation in tokens per second.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   A floating-point value representing tokens per second.
/// </returns>
/// <remarks>
///   Calculated based on output token count and elapsed generation time.
///   Reflects model performance on the current system.
/// </remarks>
function obGetSpeed(const AOllamaBox: TOllamaBox): Double; stdcall; external COllamaBoxDLL;

/// <summary>
///   Returns the total time in seconds taken by the last inference operation.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   Elapsed time in seconds, including preprocessing and generation.
/// </returns>
/// <remarks>
///   Measured with sub-second precision.
///   Useful for benchmarking and profiling generation performance.
/// </remarks>
function obGetTime(const AOllamaBox: TOllamaBox): Double; stdcall; external COllamaBoxDLL;

/// <summary>
///   Indicates whether the most recent inference operation was cancelled before completion.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   <c>True</c> if the generation was cancelled by a registered callback; otherwise, <c>False</c>.
/// </returns>
/// <remarks>
///   This flag is automatically reset at the start of each new inference cycle.
///   It reflects whether the <see cref="TobCancelCallback"/> returned <c>True</c> during token generation.
/// </remarks>
function obGetWasCancelled(const AOllamaBox: TOllamaBox): Boolean; stdcall; external COllamaBoxDLL;

/// <summary>
///   Retrieves whether the model's token output should be wrapped in &lt;think&gt;&lt;/think&gt; tags.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   <c>True</c> if thinking output tags are enabled; otherwise, <c>False</c>.
/// </returns>
/// <remarks>
///   When enabled, tokens generated during the "thinking" phase are streamed between
///   the tags <c>&lt;think&gt;</c> and <c>&lt;/think&gt;</c>, allowing the host UI or
///   parser to distinguish pre-response reasoning content.
/// </remarks>
function obGetShowThinking(const AOllamaBox: TOllamaBox): Boolean; stdcall; external COllamaBoxDLL;

/// <summary>
///   Enables or disables the wrapping of "thinking" tokens in &lt;think&gt;&lt;/think&gt; tags.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="AShowThinking">
///   <c>True</c> to enable output tagging; <c>False</c> to disable.
/// </param>
/// <remarks>
///   This affects how streamed output is formatted when the model is reasoning
///   prior to emitting a final response. It is purely cosmetic and does not affect the model's behavior.
/// </remarks>
procedure obSetShowThinking(const AOllamaBox: TOllamaBox; const AShowThinking: Boolean); stdcall; external COllamaBoxDLL;

/// <summary>
///   Returns whether the model is currently in the "thinking" phase of inference.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   <c>True</c> if the model is currently reasoning or preparing a response; otherwise, <c>False</c>.
/// </returns>
/// <remarks>
///   This flag is useful for updating UI indicators or controlling application flow
///   while the model is actively generating tokens.
/// </remarks>
function obThinking(const AOllamaBox: TOllamaBox): Boolean; stdcall; external COllamaBoxDLL;

/// <summary>
///   Retrieves whether the model's response tokens should be wrapped in &lt;response&gt;&lt;/response&gt; tags.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   <c>True</c> if response output tagging is enabled; otherwise, <c>False</c>.
/// </returns>
/// <remarks>
///   When enabled, the generated response will be enclosed between <c>&lt;response&gt;</c>
///   and <c>&lt;/response&gt;</c> tags. This helps downstream tools or UIs identify
///   the boundaries of the final answer.
/// </remarks>
function obGetShowResponding(const AOllamaBox: TOllamaBox): Boolean; stdcall; external COllamaBoxDLL;

/// <summary>
///   Enables or disables wrapping of the model's output in &lt;response&gt;&lt;/response&gt; tags.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <param name="AShowResponding">
///   <c>True</c> to enable tagging; <c>False</c> to disable.
/// </param>
/// <remarks>
///   This setting is useful for log formatting, parsing automation, or frontend highlighting.
///   It is a cosmetic wrapper and does not alter model behavior.
/// </remarks>
procedure obSetShowResponding(const AOllamaBox: TOllamaBox; const AShowResponding: Boolean); stdcall; external COllamaBoxDLL;

/// <summary>
///   Indicates whether the model is currently in the "responding" phase of inference.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   <c>True</c> if the model is actively generating a final response; otherwise, <c>False</c>.
/// </returns>
/// <remarks>
///   This can be used to detect when the response phase has begun (after thinking)
///   and is still ongoing, enabling live UI updates or stream parsing logic.
/// </remarks>
function obResponding(const AOllamaBox: TOllamaBox): Boolean; stdcall; external COllamaBoxDLL;

/// <summary>
///   Retrieves the HTTP status code from the most recent API request.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   An integer representing the HTTP status code (e.g., <c>200</c>, <c>404</c>, <c>500</c>).
/// </returns>
/// <remarks>
///   This value reflects the raw HTTP response from the last operation involving a network request,
///   such as <see cref="obPull"/> or <see cref="obWebSearch"/>. It can be used to programmatically
///   detect and handle errors or success states.
/// </remarks>
function obHttpStatusCode(const AOllamaBox: TOllamaBox): Int32; stdcall; external COllamaBoxDLL;

/// <summary>
///   Retrieves the HTTP status message or reason phrase from the most recent API request.
/// </summary>
/// <param name="AOllamaBox">
///   A handle to the <see cref="TOllamaBox"/> instance.
/// </param>
/// <returns>
///   A pointer to a null-terminated wide string containing the HTTP status text
///   (e.g., <c>"OK"</c>, <c>"Not Found"</c>, <c>"Internal Server Error"</c>).
/// </returns>
/// <remarks>
///   This message is provided by the remote server as part of the HTTP response.
///   It is useful for logging, error reporting, and human-readable diagnostics.
/// </remarks>
function obHttpStatusText(const AOllamaBox: TOllamaBox): PWideChar; stdcall; external COllamaBoxDLL;

implementation

end.
